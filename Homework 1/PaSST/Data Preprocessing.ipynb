{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import pickle \n",
    "\n",
    "import torch\n",
    "# import librosa\n",
    "sys.path.append(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_config = {\n",
    "\t'batch_size': 1,\n",
    "\t'eps': 1e-5,\n",
    "\t'sample_rate': 22050,\n",
    "\t'load_size': 22050 * 20,\n",
    "\t'name_scope': 'SoundNet_TF',\n",
    "\t'phase': 'extracttt',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(audio_path, sample_rate=22050, mono=True):\n",
    "    # By default, librosa will resample the signal to 22050Hz(sr=None). And range in (-1., 1.)\n",
    "    sound_sample, sr = librosa.load(audio_path, sr=sample_rate, mono=mono)\n",
    "    \n",
    "    assert sample_rate == sr\n",
    "\n",
    "    return sound_sample, sr\n",
    "\n",
    "def gen_audio_from_dir(dir, file_list, file_ext='.mp3', config=local_config):\n",
    "    '''Audio loader from dir generator'''\n",
    "    txt_list = []\n",
    "    \n",
    "    audio_path_list = Path(dir).glob(f'*{file_ext}')\n",
    "\n",
    "    for audio_path in tqdm(audio_path_list):\n",
    "        audio_path_str = str(audio_path).split('/')[-1][:-4]\n",
    "        if audio_path_str in file_list:\n",
    "            sound_sample, _ = load_audio(audio_path)\n",
    "            yield preprocess(sound_sample, config), audio_path \n",
    "\n",
    "def preprocess(raw_audio, config=local_config):\n",
    "    # Select first channel (mono)\n",
    "    if len(raw_audio.shape) > 1:\n",
    "        raw_audio = raw_audio[0]\n",
    "\n",
    "    # Make range [-256, 256]\n",
    "    raw_audio *= 256.0\n",
    "\n",
    "    # Make minimum length available\n",
    "    length = config['load_size']\n",
    "    if length > raw_audio.shape[0]:\n",
    "        raw_audio = np.tile(raw_audio, int(length/raw_audio.shape[0] + 1))\n",
    "\n",
    "    # Make equal training length\n",
    "    if config['phase'] != 'extract':\n",
    "        raw_audio = raw_audio[:length]\n",
    "\n",
    "    assert len(raw_audio.shape) == 1, \"Audio is not mono\"\n",
    "    assert np.max(raw_audio) <= 256, \"Audio max value beyond 256\"\n",
    "    assert np.min(raw_audio) >= -256, \"Audio min value beyond -256\"\n",
    "\n",
    "    # Shape for network is 1 x DIM x 1 x 1\n",
    "    raw_audio = np.reshape(raw_audio, [1, 1, -1, 1])\n",
    "\n",
    "    return raw_audio.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos_label = {}\n",
    "for line in open('../labels/train.csv').readlines()[1:]:\n",
    "    video_id, category = line.strip().split(\",\")\n",
    "    df_videos_label[video_id] = category\n",
    "train_file_list = list(df_videos_label.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchaudio 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 215406])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp3_file_list = os.listdir('../mp3')\n",
    "sample_audio, sr = torchaudio.load('../mp3/' + mp3_file_list[0])\n",
    "sample_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 441000, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(sample_audio).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  0.,  0.,  ..., 14., 14., 14.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../labels/train.csv')\n",
    "torch.Tensor(train_df.Category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Dataset from mp3 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mp3_dataset(Dataset):\n",
    "    def __init__(self, feat_dir, label_dir):\n",
    "\n",
    "        file_df = pd.read_csv(label_dir)\n",
    "        mp3_file_list = list(file_df.Id)\n",
    "        category_list = list(file_df.Category)\n",
    "        features = []\n",
    "        labels = []\n",
    "        error_count = 0\n",
    "\n",
    "        for file, categ in tqdm(zip(mp3_file_list, category_list)):\n",
    "            try:\n",
    "                sample_audio, sr = torchaudio.load(feat_dir + str(file) + '.mp3')\n",
    "                new_audio = preprocess(sample_audio)\n",
    "                features.append(new_audio[0, 0, :, 0])\n",
    "                labels.append(categ)\n",
    "            except:                \n",
    "                error_count += 1\n",
    "                pass\n",
    "#         features = torch.stack(features)\n",
    "        labels = torch.Tensor(labels)\n",
    "\n",
    "        if error_count > 0:\n",
    "            print(f'Could not process {error_count} audio files correctly.')\n",
    "        \n",
    "        self.length = len(features)\n",
    "        self.feats = features\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        \n",
    "        data = self.feats[ind]\n",
    "        label = self.labels[ind]\n",
    "\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2416it [00:12, 188.36it/s]formats: can't open input file `../mp3/NTkxNzA4MjE4OTM1ODg4NTYxOA==.mp3': No such file or directory\n",
      "5381it [00:28, 181.09it/s]formats: can't open input file `../mp3/LTgxOTM5Mzg2MTMwNzM4NjQzNzg=.mp3': No such file or directory\n",
      "5740it [00:30, 188.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not process 2 audio files correctly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = mp3_dataset('../mp3/', '../labels/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "241it [00:01, 191.38it/s]formats: can't open input file `../mp3/LTQ5ODI3NjU5MTQ3OTQ4NTAwOQ==.mp3': No such file or directory\n",
      "1760it [00:09, 190.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not process 1 audio files correctly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "749it [00:03, 187.83it/s]\n"
     ]
    }
   ],
   "source": [
    "val_data = mp3_dataset('../mp3/', '../labels/val.csv')\n",
    "test_data = mp3_dataset('../mp3/', '../labels/test_for_students.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 441000])\n",
      "tensor([ 7.,  1.,  4.,  1.,  1.,  7.,  8.,  6., 12.,  7.,  9.,  4.,  9.,  4.,\n",
      "         0.,  3.,  0., 13.,  9.,  0.,  9.,  2.,  2.,  0.,  5.,  0.,  5.,  6.,\n",
      "         9.,  5., 14.,  0.,  4.,  0.,  8., 12.,  4.,  5., 13.,  2.,  4., 12.,\n",
      "         1.,  6.,  9.,  2., 13.,  1.,  1.,  6., 12.,  3.,  5.,  2.,  9.,  7.,\n",
      "         6.,  4., 14.,  4.,  2., 11.,  0.,  4.])\n"
     ]
    }
   ],
   "source": [
    "for feat, l in train_loader:\n",
    "    print(feat.shape)\n",
    "    print(l)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: FMAX is None setting to 15000 \n",
      "\n",
      "\n",
      " Loading PASST TRAINED ON AUDISET \n",
      "\n",
      "\n",
      "PaSST(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pre_logits): Identity()\n",
      "  (head): Sequential(\n",
      "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=768, out_features=527, bias=True)\n",
      "  )\n",
      "  (head_dist): Linear(in_features=768, out_features=527, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [03:38<00:00, 18.22s/it]\n"
     ]
    }
   ],
   "source": [
    "from hear21passt.base import load_model, get_scene_embeddings, get_timestamp_embeddings\n",
    "\n",
    "model = load_model().cuda()\n",
    "seconds = 20\n",
    "passt_feat_all = []\n",
    "labels_all = []\n",
    "\n",
    "for feat, label in tqdm(train_loader):\n",
    "    audio = feat.to(device)\n",
    "    embed, time_stamps = get_timestamp_embeddings(audio, model)\n",
    "#     print(embed.shape)\n",
    "    embed = get_scene_embeddings(audio, model)\n",
    "#     print(embed.shape)\n",
    "    passt_feat_all.append(embed)\n",
    "    labels_all.append(label)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([749, 1295])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.vstack(passt_feat_all).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "passt_feat_all = torch.vstack(passt_feat_all)\n",
    "labels_all = torch.cat(labels_all)\n",
    "\n",
    "with open('passt_feat_train.pkl', 'wb') as f:\n",
    "    pickle.dump(passt_feat_all, f)\n",
    "with open('passt_label_train.pkl', 'wb') as f:\n",
    "    pickle.dump(labels_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passt_feat_all = []\n",
    "labels_all = []\n",
    "\n",
    "for feat, label in tqdm(val_loader):\n",
    "    audio = feat.to(device)\n",
    "    embed, time_stamps = get_timestamp_embeddings(audio, model)\n",
    "#     print(embed.shape)\n",
    "    embed = get_scene_embeddings(audio, model)\n",
    "#     print(embed.shape)\n",
    "    passt_feat_all.append(embed)\n",
    "    labels_all.append(label)\n",
    "    \n",
    "    \n",
    "passt_feat_all = torch.vstack(passt_feat_all)\n",
    "labels_all = torch.cat(labels_all)\n",
    "\n",
    "with open('passt_feat_val.pkl', 'wb') as f:\n",
    "    pickle.dump(passt_feat_all, f)\n",
    "with open('passt_label_val.pkl', 'wb') as f:\n",
    "    pickle.dump(labels_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passt_feat_all = []\n",
    "labels_all = []\n",
    "\n",
    "for feat, label in tqdm(test_loader):\n",
    "    audio = feat.to(device)\n",
    "    embed, time_stamps = get_timestamp_embeddings(audio, model)\n",
    "#     print(embed.shape)\n",
    "    embed = get_scene_embeddings(audio, model)\n",
    "#     print(embed.shape)\n",
    "    passt_feat_all.append(embed)\n",
    "    labels_all.append(label)\n",
    "    \n",
    "    \n",
    "passt_feat_all = torch.vstack(passt_feat_all)\n",
    "labels_all = torch.cat(labels_all)\n",
    "\n",
    "with open('passt_feat_test.pkl', 'wb') as f:\n",
    "    pickle.dump(passt_feat_all, f)\n",
    "with open('passt_label_test.pkl', 'wb') as f:\n",
    "    pickle.dump(labels_all, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a57ea99128cb9cda106ae9412adaa259fbd6364806b681dd046ce8028342a29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
